<pre> A new streaming consumer needs access to HDInsight Kafka through an Azure Databricks notebook. To set this up, you need the IP addresses for connections. Which IP addresses are required to set up a streaming connection for consumers? </pre>
<ul>
	<li> <b>Kafka cluster </b></li>
	<li> Databricks cluster </li>
	<li> Azure virtual network gateway </li>
	<li> Kafka ZooKeeper </li>
</ul>


<pre> When setting up virtual network perring for an Azure Databricks instance to a Kafka cluster, where do you add the peering connection? </pre>
<ul>
	<li> Window Admin Center</li>
	<li> <b>HDinsight portal</b> </li>
	<li> Azure Databricks workspace portal</li>
	<li> Azure Cloud Shell </li>
</ul>



<pre> You are authoring a notebook in Azure Databricks to produce twitter data into an existing topic called Tweets in HDInsight kafka. When you attempt to make the conenction, it will nto work. What step must you complete in Azure Databricks workspace portal during the setup of the streaming data components?</pre>
<ul>
	<li> Set up virtual network peering between the cluster</li>
	<li> <b>Add the spark-streaming twitter library for Twitter to the Spark cluster </b></li>
	<li> AConfigure the kafka cluster to use IP addresses</li>
	<li> Add the Tweets topic to Kafka </li>
</ul>


<pre> While building a machine learning model in a Databricks notebook, you need to access to the Apache Spark machine learning library for tools and utilities. What method would you use to acces this library when running a notebook?</pre>
<ul>
	<li> Add the library to the Databricks cluster</li>
	<li> <b>Import the library in a code cell </b></li>
	<li> Import the library into your virtual environemnt in Azure Cloud shell</li>
	<li> Import the lbirary in a GitHub repository in Azure Cloud Shell </li>
</ul>


<pre> You must deploy the code for traning a machine learning model in a batch scoring pipeline in Azure Databricks. Where should you enter the code for the model building process?</pre>
<ul>
	<li> Azure cloud Shell</li>
	<li>  Databricks CLI</li>
	<li> Cluster library</li>
	<li> <b>Notebook cell </b> </li>
</ul>



<pre> When dealing with a large data set, you must refine the data into a smaller subset of data for an analytics dashboard. During which phase of the extract, transform, load (ETL) process would you perform this?</pre>
<ul>
	<li> Ingest</li>
	<li> Extract</li>
	<li> Load</li>
	<li> <b>Transform</b> </li>
</ul>

<pre> When transforming a raw data set into a subset of data stored in a data frame, which command is part of data frame creation process?</pre>
<ul>
	<li> .read()</li>
	<li> .show()</li>
	<li> .write()</li>
	<li> <b>.select() </b> </li>
</ul>



<pre> A folder has been created in an Azure Databricks workspace called New-Project. Access must be given to a group of five data scienties so they can run and modify code in notebooks located in the folder. They must also be prevented from delting any assets in the folder. What is the best way to give your data scientists the access they need</pre>
<ul>
	<li> Add each user with Can Read permission to the new-rpoject folder.</li>
	<li> Add each user with can run permission to the ned project folder</li>
	<li> add each user with can manager permission to the new project folder</li>
	<li> <b>add each user with can edit permission to the new project folder. </b></li>
</ul>


<pre>notebooks supported multiple languages and formats in Azure Databricks. one of the popular formats for adding documentation such as text and images to a notebook is using markdown. To add markdown documentation to a cell in a Python notebook, what must oyu add to the top of the cell?</pre>
<ul>
	<li>  --md</li>
	<li> //md</li>
	<li> <b>%md </b></li>
	<li> #md</li>
</ul>


<pre> You must create a job in Azure Databricks. You have a JSON file with all the instructions for the job and it has been loaded into Azure Cloud Shell with a local path of clouddrive/demo/bd-job01.json. Which commands will create the job?</pre>
<ul>
	<li>  databricks jobs reate --json-file dbfs:/clouddrive/demo/bd-job01.json</li>
	<li> databricks jobs create --json clouddrive/demo/bd-job01.json</li>
	<li> databricks jobs create --json dbfs:/clouddrive/demo/bd-job01.json</li>
	<li><b> databricks jobs create --json-file clouddrive/demo/bd-job01.json </b></li>
</ul>


